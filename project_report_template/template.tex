\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\usepackage{setspace}
\usepackage{enumerate}
\usepackage{cite}
\usepackage{times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}

\begin{document}
\title{Hybrid Implementation of Expectation Maximization Algorithm using CUDA and OpenMP for Gaussian Mixture Models}

\author{
\IEEEauthorblockN{
Ayush Raina
}
\IEEEauthorblockA{Supercomputer~Education~and~Research~Centre\\
Indian Institute of Science, Bangalore, India\\
ayushraina@iisc.ac.in}
}

\maketitle

\begin{abstract}
    The Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMMs) presents substantial computational challenges for large-scale, high-dimensional datasets. We introduce a novel heterogeneous computing approach that synergistically combines GPU acceleration via CUDA with CPU parallelization using OpenMP. Our architecture strategically offloads the computation-intensive E-step to the GPU while executing the more synchronization-dependent M-step on multi-core CPUs, achieving optimal hardware utilization. Numerical stability in high dimensions is ensured through sophisticated log-space computations and adaptive regularization techniques. Performance analysis on synthetic datasets demonstrates remarkable speedups—approximately 73× over sequential implementations and significantly outperforming both pure-CPU (8-9×) and pure-GPU (4×) implementations. The hybrid design effectively balances computational throughput with memory transfer overhead, scales efficiently with increasing dataset size and model complexity, and exhibits superior convergence characteristics. This architecture represents a practical solution for accelerating mixture model estimation in data-intensive applications where both computational efficiency and numerical robustness are critical constraints.
\end{abstract}

\section{Introduction}
\label{intro}

Gaussian Mixture Models (GMMs) are powerful probabilistic models used extensively in machine learning for density estimation, clustering, classification, and anomaly detection. GMMs represent complex data distributions as a weighted sum of multivariate Gaussian distributions:

\begin{equation}
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}

\noindent where $\pi_k$ represents the mixture weights (which sum to 1), $\boldsymbol{\mu}_k$ is the mean vector, and $\boldsymbol{\Sigma}_k$ is the covariance matrix of the $k$-th component.

The Expectation-Maximization (EM) algorithm is the standard approach for estimating GMM parameters, iteratively refining model parameters to maximize the data likelihood. The EM algorithm consists of two main steps: (1) the Expectation step (E-step), which computes posterior probabilities (responsibilities) for each data point belonging to each Gaussian component, and (2) the Maximization step (M-step), which updates model parameters based on these responsibilities.

For large datasets with high dimensionality, these computations become extremely intensive, with the E-step and M-step both scaling as \textcolor{blue}{$O(NKD^2)$} for $N$ samples, $K$ components, and $D$ dimensions per iteration until convergence. The M-step has lower computational complexity at \textcolor{blue}{$O(NKD)$} if diagonal covariance matrices are considered and it involves more complex dependencies between data points. This computational burden makes acceleration through parallel processing essential for practical applications. While previous approaches have typically focused on implementing the entire EM algorithm on either CPUs or GPUs or Multi-GPU, we propose a hybrid approach that strategically utilizes a \textbf{single GPU} alongside a \textbf{multi-core CPU}, mapping different parts of the algorithm to the hardware best suited for that computation.

In this paper, we present a hybrid implementation that:
\begin{enumerate}
    \item Maps the computationally intensive, data-parallel E-step to the GPU using CUDA
    \item Leverages multi-core CPUs with OpenMP for the more synchronization-heavy M-step
    \item Addresses critical numerical stability challenges in high-dimensional GMM implementations through log-space computations
    \item Minimizes data transfer between CPU and GPU, a common bottleneck in heterogeneous computing
    \item Implements dimension-aware regularization techniques for robust covariance estimation
\end{enumerate}

The rest of the paper is organized as follows: Section~\ref{methodology} details the methodology and hybrid algorithm design, including workload analysis, parallelization strategies, and implementation details. Section~\ref{experiments} presents experimental results and performance analysis. Section~\ref{conclusion} concludes the paper and discusses directions for future work.

\section{Methodology}
\label{methodology}

\subsection{GMM Background and EM Algorithm}

The log-likelihood function for a GMM with $K$ components for a dataset $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\}$ is:

\begin{equation}
\log p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)
\end{equation}

The EM algorithm iteratively maximizes this log-likelihood by alternating between two steps:

\textbf{E-step}: Compute the posterior probability (responsibility) that component $k$ takes for explaining data point $\mathbf{x}_i$:

\begin{equation}
\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
\end{equation}

\textbf{M-step}: Update the model parameters using computed responsibilities:

\begin{align}
\pi_k^{new} &= \frac{N_k}{N} \quad \text{where} \quad N_k = \sum_{i=1}^{N} \gamma_{ik} \\
\boldsymbol{\mu}_k^{new} &= \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i \\
\boldsymbol{\Sigma}_k^{new} &= \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k^{new})(\mathbf{x}_i - \boldsymbol{\mu}_k^{new})^T
\end{align}

The algorithm iterates until convergence, typically determined by monitoring changes in log-likelihood between iterations.

\subsection{Algorithm Overview}

Our hybrid implementation follows this high-level workflow:

\begin{algorithm}[h]
\caption{Hybrid CUDA-OpenMP EM Algorithm for GMMs}
\begin{algorithmic}[1]
\STATE Initialize model parameters on CPU: $\boldsymbol{\pi}$, $\boldsymbol{\mu}$, $\boldsymbol{\Sigma}$
\STATE Precompute precision matrices $\boldsymbol{\Sigma}^{-1}_k$ and normalizer terms $c_k = -\frac{D}{2}\ln(2\pi) - \frac{1}{2}\ln|\boldsymbol{\Sigma}_k|$
\WHILE{not converged and iterations $<$ max\_iterations}
    \STATE Transfer model parameters to GPU
    \STATE Execute E-step on GPU using log-space CUDA kernels:
    \begin{align}
        \ln p(x_i | k) &= \ln\pi_k + c_k - \frac{1}{2}(x_i-\mu_k)^T \boldsymbol{\Sigma}_k^{-1} (x_i-\mu_k)
    \end{align}
    \STATE Calculate responsibilities $\gamma_{ik}$ using the log-sum-exp trick for numerical stability
    \STATE Transfer responsibilities $\gamma_{ik}$ and log-likelihood $\mathcal{L}$ to CPU
    \STATE Perform M-step using OpenMP on CPU to update parameters (as described in Section \ref{methodology})
    \STATE Check convergence: $|\mathcal{L}_{\text{new}} - \mathcal{L}_{\text{old}}| < \epsilon$
    \IF{not converged}
        \STATE Precompute updated precision matrices and normalizers
    \ENDIF
\ENDWHILE
\RETURN Final model parameters $\boldsymbol{\pi}$, $\boldsymbol{\mu}$, $\boldsymbol{\Sigma}$
\end{algorithmic}
\end{algorithm}

The algorithm iteratively refines GMM parameters by delegating the computationally intensive E-step to the GPU while performing parameter updates in the M-step on the CPU. This division leverages the strengths of each hardware component.

\subsection{E-step: GPU Implementation with Log-Space Computation}

The E-step is the most computationally intensive part of the EM algorithm, especially for high-dimensional data. Our GPU implementation focuses on both performance and numerical stability.

\subsubsection{Log-Space Computation for Numerical Stability}

Computing Gaussian probabilities in high dimensions can quickly lead to numerical underflow due to the exponential term with potentially large negative exponents. We implement log-space computation as follows:

The log of the multivariate Gaussian PDF is:

\begin{equation}
\log \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Sigma}) = -\frac{D}{2}\log(2\pi) - \frac{1}{2}\log|\boldsymbol{\Sigma}| - \frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})
\end{equation}

For efficiency, we precompute the precision matrices $\boldsymbol{\Sigma}^{-1}$ and the normalizer terms $-\frac{D}{2}\log(2\pi) - \frac{1}{2}\log|\boldsymbol{\Sigma}|$ on the CPU, allowing us to reuse these values for all data points.



After computing log probabilities, we use the log-sum-exp trick to compute responsibilities while maintaining numerical stability:

\begin{algorithm}
    \caption{CUDA Kernel: Calculate Responsibilities}
    \begin{algorithmic}[1]
    \STATE $i \gets \textrm{thread index corresponding to data point}$
    \IF{$i < \textrm{n\_samples}$}
        \STATE $\textrm{max\_log\_prob} \gets -\infty$ \COMMENT{Initialize to negative infinity}
        \FOR{$k = 0 \textrm{ to } \textrm{n\_components}-1$}
            \STATE $\textrm{max\_log\_prob} \gets \max(\textrm{max\_log\_prob}, \textrm{log\_probs}[i, k])$
        \ENDFOR
        
        \STATE $\textrm{sum\_exp} \gets 0.0$
        \FOR{$k = 0 \textrm{ to } \textrm{n\_components}-1$}
            \STATE $\textrm{sum\_exp} \gets \textrm{sum\_exp} + \exp(\textrm{log\_probs}[i, k] - \textrm{max\_log\_prob})$
        \ENDFOR
        
        \STATE $\textrm{log\_sum\_exp} \gets \textrm{max\_log\_prob} + \log(\textrm{sum\_exp})$
        \STATE $\textrm{log\_likelihood}[i] \gets \textrm{log\_sum\_exp}$ \COMMENT{Store for convergence check}
        
        \FOR{$k = 0 \textrm{ to } \textrm{n\_components}-1$}
            \STATE $\textrm{resp}[i, k] \gets \exp(\textrm{log\_probs}[i, k] - \textrm{log\_sum\_exp})$
        \ENDFOR
    \ENDIF
    \end{algorithmic}
    \end{algorithm}

The log-sum-exp trick involves finding the maximum log probability for numerical stability:

\begin{equation}
\log\sum_{k} \exp(a_k) = m + \log\sum_{k} \exp(a_k - m)
\end{equation}

\noindent where $m = \max_k a_k$. This prevents underflow by shifting values to a numerically stable range.

\subsubsection{GPU Memory Access Optimization}

Our CUDA implementation prioritizes efficient memory access patterns in our numerically stable log-space kernels:

\begin{itemize}
    \item \textbf{Coalesced Global Memory Access}: Data is stored in row-major layout to ensure adjacent threads access adjacent memory locations. In our GPU kernels, array indexing follows patterns like "\textcolor{blue}{log\_probs[sample\_idx $\times$ n\_components + k]}", maximizing memory bandwidth utilization.
    
    \item \textbf{Register Optimization}: Critical variables such as "\textcolor{blue}{max\_log\_prob", "sum\_exp}", and intermediate calculation results are kept in local variables (automatically stored in registers) to minimize latency in our calculation-intensive kernels.
    
    \item \textbf{Memory Layout Conversion}: We implement specialized conversion functions that efficiently transform between Eigen's column-major format on the CPU and the row-major format optimized for CUDA operations via our \texttt{\textcolor{blue}{eigenToArray}} and \texttt{\textcolor{blue}{arrayToEigen}} utility functions.
    
    \item \textbf{Precomputation Caching}: We precompute and cache precision matrices and normalizers on the CPU, transferring these values to the GPU only once per iteration to minimize redundant calculations and memory transfers.
\end{itemize}

Our implementation carefully manages data structures to match the computational patterns of log-space operations, ensuring efficient access to precomputed terms during the calculation of Gaussian probabilities and responsibilities.

\subsubsection{CUDA Kernel Configuration}

Our implementation utilizes two specialized CUDA kernels for the E-step, one for computing log probabilities and other for converting these to responsibilities using the log-sum-exp trick. We choosed standard block size of 256 threads per block to provide the best balance between occupancy and memory usage.

\subsection{M-step: OpenMP Implementation}

The M-step updates model parameters using the responsibilities calculated in the E-step. We implement this using OpenMP to leverage CPU parallelism:

\begin{algorithm}
\caption{M-step with OpenMP Parallelization}
\begin{algorithmic}[1]
\STATE $N_k \gets$ sum of responsibilities for each component
\STATE \#pragma omp parallel for
\FOR{$k = 0 \textrm{ to } n\_components-1$}
    \STATE $\pi[k] \gets N_k[k] / n\_samples$
\ENDFOR

\STATE \#pragma omp parallel for
\FOR{$k = 0 \textrm{ to } n\_components-1$}
    \IF{$N_k[k] > 0$}
        \STATE Initialize $\mu[k]$ and $\Sigma[k]$ to zero
        \FOR{$i = 0 \textrm{ to } n\_samples-1$}
            \STATE $\mu[k] \gets \mu[k] + \text{resp}[i,k] \times X[i]$
        \ENDFOR
        \STATE $\mu[k] \gets \mu[k] / N_k[k]$
        
        \FOR{$i = 0 \textrm{ to } n\_samples-1$}
            \STATE $\text{diff} \gets X[i] - \mu[k]$
            \STATE $\Sigma[k] \gets \Sigma[k] + \text{resp}[i,k] \times \text{diff} \times \text{diff}^T$
        \ENDFOR
        \STATE $\Sigma[k] \gets \Sigma[k] / N_k[k] + \text{reg\_factor} \times I$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

Our implementation parallelizes component-wise operations, as each component's parameters can be updated independently. We use OpenMP's `parallel for` directive to distribute iterations across CPU threads, with dynamic scheduling for better load balancing when component update times vary. 

\subsection{Precomputation of Precision Terms}

After each M-step update, we precompute precision matrices and normalizer terms for the next E-step:

\begin{algorithm}
\caption{Precomputation of Precision Terms}
\begin{algorithmic}[1]
\STATE \#pragma omp parallel for
\FOR{$k = 0$ \TO $n\_components-1$}
    \STATE Apply covariance regularization to $\Sigma[k]$
    \STATE $\Sigma^{-1}[k] \gets$ inverse of $\Sigma[k]$
    \STATE $normalizers[k] \gets -\frac{D}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma[k]|$
\ENDFOR
\end{algorithmic}
\end{algorithm}

This precomputation significantly reduces the computational load in the E-step, as these terms remain constant for all data points within an iteration. In the actual implementation of above function, we have taken care of numerical instabilities.

\subsection{Implementation Details}

Our implementation uses C++ with CUDA for GPU acceleration and OpenMP for CPU parallelization, with Eigen for matrix operations. We developed specialized conversion functions between Eigen's column-major format and CUDA's row-major layout to ensure optimal memory access patterns.

The implementation was tested on an Intel Core i9-12900K (16 cores, 24 threads) with NVIDIA GeForce GTX 1660 (6GB) GPU. Our OpenMP implementation automatically scales to utilize all available CPU cores, providing optimal performance across different system configurations. This design minimizes transfer overhead between CPU and GPU while optimizing cache utilization on both architectures.

\section{Experiments and Results}

\subsection{Experiment 1: Synthetic Data and Performance Evaluation}
In this experiment, we evaluate the performance of our hybrid implementation on synthetic datasets of varying sizes (2M, 4M, 6M, 8M, and 10M data points) with 5 clusters and 10-dimensional data.
\begin{figure}[h!]
\centering
\includegraphics[width=0.25\textwidth]{executionTimes1.png}
\includegraphics[width=0.2\textwidth]{speedups.png}
\caption{Execution times and Speedups over sequential implementation are shown.}
\end{figure}
We run 5 iterations of each variant here and the data reveals impressive performance gains from parallel implementations across different dataset sizes. The sequential version shows poor scalability, with execution time increasing from $\sim$117s to $\sim$568s as data grows.

For the E-step, GPU implementation excels with $\sim$220-234x speedup, while CPU parallelization achieves only $\sim$9.8x. In contrast, the M-step shows reversed performance patterns with GPU performing poorly ($\sim$0.1x) and CPU showing moderate improvement ($\sim$4.0x).

The hybrid approach effectively combines the strengths of both platforms, using GPU for the computation-heavy E-step and CPU for the more sequential M-step. This strategic combination delivers the best overall performance with total speedups of 70-74x, significantly outperforming either GPU-only ($\sim$4.0x) or CPU-only ($\sim$9.0x) implementations.

These results highlight the importance of matching algorithm components to the most suitable hardware architecture for maximum performance gains in large-scale data processing.

\subsection{Experiment 2: Comparison with Azizi's Paper}
We generated a synthetic dataset of 1 million points as specified in Azizi's paper and compared our hybrid implementation with the Python, NumPy, Numba, and CuPy implementations provided in the paper. We found that when starting the GMM fitting process with the initial parameters given in the paper, neither their implementations nor our hybrid version converged within 2000+ iterations. However, when initialized with the same random parameters, our hybrid implementation converged in lesser iterations than all other implementations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.25\textwidth]{azizi1.png}
    \includegraphics[width=0.2\textwidth]{azizi2.png}
    \caption{Left: Log-likelihood when starting with the initial parameters from the paper. Right: Log-likelihood when starting with random parameters.}
\end{figure}

Since Numba and CuPy are highly optimized libraries, their iteration speed is much faster than our implementation because of vectorization and other optimizations. If we run their code with their initial parameters and our code as usual, we get the following plot, in which we converge much early:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\textwidth]{azizi3.png}
\end{figure}

\subsection*{Experiment 3: Testing our Hybrid Algorithm on IRIS Dataset}

To validate our hybrid algorithm on real-world data, we tested it on the classic Iris dataset, which contains 150 samples from three iris species with four features each (sepal length, sepal width, petal length, and petal width). The dataset presents an excellent benchmark for clustering algorithms due to its well-defined structure with one linearly separable class and two partially overlapping classes.

Our hybrid GMM implementation achieved excellent clustering results on this dataset, as evidenced by multiple evaluation metrics: Misclassification Rate (MCR) of 0.03333, Adjusted Rand Index (ARI) of 0.90387, and F-measure of 0.96658. These metrics indicate that our algorithm correctly identified the underlying clusters with high accuracy, making only 5 misclassifications out of 150 samples.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\textwidth]{iris1.png}
    \caption{Top row shows the true class labels across three feature pairs. Bottom row displays the fitted GMM clusters with corresponding density contours. The close match between true labels and model predictions demonstrates the algorithm's effectiveness at recovering the natural clusters in the data.}
    \label{fig:iris_results}
\end{figure}

\section{Conclusion and Future Work}
\label{conclusion}

In this paper, we presented a hybrid parallel implementation of the EM algorithm for GMMs that effectively leverages both GPU and CPU resources. The hybrid approach demonstrates very good performance compared to pure CPU and pure GPU implementations, with performance advantages that scale with dataset size and dimensionality. Details of the regularization techniques are not discussed here for brevity but can be found in our implementation.

Future work could explore several directions:
\begin{enumerate}
    \item \textbf{Optimized Linear Algebra Libraries}: Replacing Eigen with CUDA-optimized libraries like cuBLAS and cuSOLVER for matrix operations to further accelerate covariance computations and matrix inversions.
    
    \item \textbf{Sparse GMMs}: Implementing sparse covariance matrix representations for very high-dimensional data.
    
    \item \textbf{Integration with Deep Learning}: Exploring hybrid deep learning models that incorporate GMMs as components.
\end{enumerate}

Our implementation demonstrates that effectively leveraging heterogeneous computing resources can significantly accelerate machine learning algorithms, enabling the analysis of larger and higher-dimensional datasets than previously practical.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{1}

\bibitem{azizi2023parallel}
I.~Azizi, ``Parallelization in Python -- An Expectation-Maximization Application,'' Département des Opérations, Université de Lausanne, Jun. 2023. [Online]. Available: \url{https://iliaazizi.com/projects/em_parallelized/report.pdf}

\bibitem{kumar2009fast}
N.~S.~L. Phani Kumar, S. Satoor, and I. Buck, ``Fast parallel expectation maximization for Gaussian mixture models on GPUs using CUDA,'' in \emph{11th IEEE International Conference on High Performance Computing and Communications (HPCC)}, 2009, pp.~103--109. [Online]. Available: \url{https://ieeexplore.ieee.org/document/5166982}

\end{thebibliography}



\end{document}